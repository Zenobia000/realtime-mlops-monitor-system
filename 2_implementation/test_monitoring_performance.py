"""
ç›£æ§æ””æˆªå™¨æ€§èƒ½æ¸¬è©¦è…³æœ¬
é©—è­‰ç›£æ§ä¸­é–“ä»¶çš„æ€§èƒ½å½±éŸ¿æ˜¯å¦ç¬¦åˆ WBS è¦æ±‚ (< 20ms)
"""

import asyncio
import time
import statistics
from typing import List, Dict, Any
import aiohttp
import json
from datetime import datetime

# æ¸¬è©¦é…ç½®
TEST_CONFIG = {
    "base_url": "http://localhost:8002",
    "concurrent_requests": [1, 10, 50, 100],
    "test_endpoints": [
        {"path": "/predict", "method": "POST", "data": {"features": [1.0, 2.0, 3.0]}},
        {"path": "/health", "method": "GET", "data": None},
        {"path": "/models", "method": "GET", "data": None},
        {"path": "/slow_endpoint", "method": "GET", "params": {"delay": 0.1}},
    ],
    "requests_per_test": 100,
    "timeout_seconds": 30
}


class PerformanceTestResult:
    """æ€§èƒ½æ¸¬è©¦çµæœ"""
    
    def __init__(self, endpoint: str, concurrent_users: int):
        self.endpoint = endpoint
        self.concurrent_users = concurrent_users
        self.response_times: List[float] = []
        self.status_codes: List[int] = []
        self.errors: List[str] = []
        self.start_time: float = 0
        self.end_time: float = 0
    
    def add_result(self, response_time: float, status_code: int, error: str = None):
        """æ·»åŠ å–®å€‹è«‹æ±‚çµæœ"""
        self.response_times.append(response_time)
        self.status_codes.append(status_code)
        if error:
            self.errors.append(error)
    
    def get_statistics(self) -> Dict[str, Any]:
        """è¨ˆç®—çµ±è¨ˆæ•¸æ“š"""
        if not self.response_times:
            return {"error": "No response times recorded"}
        
        total_time = self.end_time - self.start_time
        successful_requests = sum(1 for code in self.status_codes if 200 <= code < 300)
        
        return {
            "endpoint": self.endpoint,
            "concurrent_users": self.concurrent_users,
            "total_requests": len(self.response_times),
            "successful_requests": successful_requests,
            "failed_requests": len(self.response_times) - successful_requests,
            "success_rate": (successful_requests / len(self.response_times)) * 100,
            "total_test_time_seconds": total_time,
            "requests_per_second": len(self.response_times) / total_time if total_time > 0 else 0,
            "response_times_ms": {
                "min": min(self.response_times) * 1000,
                "max": max(self.response_times) * 1000,
                "mean": statistics.mean(self.response_times) * 1000,
                "median": statistics.median(self.response_times) * 1000,
                "p95": statistics.quantiles(self.response_times, n=20)[18] * 1000,  # 95th percentile
                "p99": statistics.quantiles(self.response_times, n=100)[98] * 1000,  # 99th percentile
            },
            "errors": self.errors[:10],  # åªé¡¯ç¤ºå‰ 10 å€‹éŒ¯èª¤
            "middleware_overhead_check": {
                "mean_response_time_ms": statistics.mean(self.response_times) * 1000,
                "meets_requirement": statistics.mean(self.response_times) * 1000 < 20,  # WBS è¦æ±‚ < 20ms
                "requirement": "< 20ms additional overhead"
            }
        }


async def make_request(session: aiohttp.ClientSession, endpoint_config: Dict[str, Any]) -> tuple:
    """
    ç™¼é€å–®å€‹ HTTP è«‹æ±‚
    
    Returns:
        tuple: (response_time, status_code, error_message)
    """
    url = f"{TEST_CONFIG['base_url']}{endpoint_config['path']}"
    method = endpoint_config['method']
    data = endpoint_config.get('data')
    params = endpoint_config.get('params')
    
    start_time = time.perf_counter()
    
    try:
        if method == "GET":
            async with session.get(url, params=params, timeout=TEST_CONFIG['timeout_seconds']) as response:
                await response.text()  # ç¢ºä¿å®Œå…¨è®€å–éŸ¿æ‡‰
                response_time = time.perf_counter() - start_time
                return response_time, response.status, None
        
        elif method == "POST":
            headers = {"Content-Type": "application/json"}
            async with session.post(
                url, 
                json=data, 
                headers=headers,
                timeout=TEST_CONFIG['timeout_seconds']
            ) as response:
                await response.text()
                response_time = time.perf_counter() - start_time
                return response_time, response.status, None
                
    except asyncio.TimeoutError:
        response_time = time.perf_counter() - start_time
        return response_time, 408, "Request timeout"
    except Exception as e:
        response_time = time.perf_counter() - start_time
        return response_time, 500, str(e)


async def run_concurrent_test(
    endpoint_config: Dict[str, Any], 
    concurrent_users: int, 
    requests_per_user: int
) -> PerformanceTestResult:
    """
    é‹è¡Œä¸¦ç™¼æ€§èƒ½æ¸¬è©¦
    
    Args:
        endpoint_config: ç«¯é»é…ç½®
        concurrent_users: ä½µç™¼ç”¨æˆ¶æ•¸
        requests_per_user: æ¯å€‹ç”¨æˆ¶çš„è«‹æ±‚æ•¸
        
    Returns:
        PerformanceTestResult: æ¸¬è©¦çµæœ
    """
    result = PerformanceTestResult(endpoint_config['path'], concurrent_users)
    
    print(f"ğŸ”„ æ¸¬è©¦ {endpoint_config['path']} - {concurrent_users} ä½µç™¼ç”¨æˆ¶, æ¯ç”¨æˆ¶ {requests_per_user} è«‹æ±‚")
    
    async with aiohttp.ClientSession() as session:
        result.start_time = time.perf_counter()
        
        # å‰µå»ºæ‰€æœ‰ä»»å‹™
        tasks = []
        for user in range(concurrent_users):
            for request_num in range(requests_per_user):
                task = asyncio.create_task(make_request(session, endpoint_config))
                tasks.append(task)
        
        # åŸ·è¡Œæ‰€æœ‰ä»»å‹™
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        result.end_time = time.perf_counter()
        
        # è™•ç†çµæœ
        for response in responses:
            if isinstance(response, Exception):
                result.add_result(0, 500, str(response))
            else:
                response_time, status_code, error = response
                result.add_result(response_time, status_code, error)
    
    return result


async def test_monitoring_overhead():
    """
    æ¸¬è©¦ç›£æ§ä¸­é–“ä»¶çš„æ€§èƒ½é–‹éŠ·
    
    é€™å€‹æ¸¬è©¦æœƒå‘æ¸¬è©¦ API ç™¼é€è«‹æ±‚ï¼Œä¸¦æ¸¬é‡éŸ¿æ‡‰æ™‚é–“
    ä»¥é©—è­‰ç›£æ§ä¸­é–“ä»¶æ˜¯å¦ç¬¦åˆ < 20ms é¡å¤–é–‹éŠ·çš„è¦æ±‚
    """
    print("ğŸš€ é–‹å§‹ç›£æ§æ€§èƒ½æ¸¬è©¦")
    print(f"ğŸ“Š æ¸¬è©¦ç›®æ¨™: é©—è­‰ç›£æ§ä¸­é–“ä»¶é¡å¤–é–‹éŠ· < 20ms")
    print(f"ğŸ¯ æ¸¬è©¦æœå‹™: {TEST_CONFIG['base_url']}")
    print("-" * 60)
    
    all_results = []
    
    # æ¸¬è©¦æ¯å€‹ç«¯é»
    for endpoint_config in TEST_CONFIG['test_endpoints']:
        print(f"\nğŸ“ æ¸¬è©¦ç«¯é»: {endpoint_config['path']}")
        
        # æ¸¬è©¦ä¸åŒä½µç™¼ç´šåˆ¥
        for concurrent_users in TEST_CONFIG['concurrent_requests']:
            requests_per_user = max(1, TEST_CONFIG['requests_per_test'] // concurrent_users)
            
            try:
                result = await run_concurrent_test(
                    endpoint_config, 
                    concurrent_users, 
                    requests_per_user
                )
                
                stats = result.get_statistics()
                all_results.append(stats)
                
                # è¼¸å‡ºé—œéµæŒ‡æ¨™
                print(f"  ä½µç™¼ {concurrent_users:3d}: "
                      f"å¹³å‡ {stats['response_times_ms']['mean']:6.2f}ms, "
                      f"P95 {stats['response_times_ms']['p95']:6.2f}ms, "
                      f"æˆåŠŸç‡ {stats['success_rate']:5.1f}%")
                
                # æª¢æŸ¥æ˜¯å¦ç¬¦åˆæ€§èƒ½è¦æ±‚
                if stats['response_times_ms']['mean'] > 20:
                    print(f"  âš ï¸ è­¦å‘Š: å¹³å‡éŸ¿æ‡‰æ™‚é–“è¶…é 20ms è¦æ±‚")
                
            except Exception as e:
                print(f"  âŒ æ¸¬è©¦å¤±æ•—: {e}")
    
    # ç”Ÿæˆç¸½çµå ±å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“Š æ¸¬è©¦ç¸½çµå ±å‘Š")
    print("=" * 60)
    
    generate_summary_report(all_results)
    
    # ä¿å­˜è©³ç´°çµæœåˆ°æ–‡ä»¶
    save_results_to_file(all_results)


def generate_summary_report(results: List[Dict[str, Any]]):
    """ç”Ÿæˆæ¸¬è©¦ç¸½çµå ±å‘Š"""
    if not results:
        print("âŒ æ²’æœ‰æ¸¬è©¦çµæœ")
        return
    
    # è¨ˆç®—æ•´é«”çµ±è¨ˆ
    all_response_times = []
    overhead_violations = []
    high_concurrency_results = []
    
    for result in results:
        if 'response_times_ms' in result:
            all_response_times.append(result['response_times_ms']['mean'])
            
            # æª¢æŸ¥æ˜¯å¦é•å 20ms è¦æ±‚
            if result['response_times_ms']['mean'] > 20:
                overhead_violations.append({
                    'endpoint': result['endpoint'],
                    'concurrent_users': result['concurrent_users'],
                    'mean_response_time': result['response_times_ms']['mean']
                })
            
            # æ”¶é›†é«˜ä½µç™¼æ¸¬è©¦çµæœ
            if result['concurrent_users'] >= 50:
                high_concurrency_results.append(result)
    
    if all_response_times:
        print(f"ğŸ“ˆ æ•´é«”æ€§èƒ½æŒ‡æ¨™:")
        print(f"   å¹³å‡éŸ¿æ‡‰æ™‚é–“: {statistics.mean(all_response_times):.2f}ms")
        print(f"   æœ€ä½éŸ¿æ‡‰æ™‚é–“: {min(all_response_times):.2f}ms")
        print(f"   æœ€é«˜éŸ¿æ‡‰æ™‚é–“: {max(all_response_times):.2f}ms")
    
    # æª¢æŸ¥æ€§èƒ½è¦æ±‚ç¬¦åˆæƒ…æ³
    if overhead_violations:
        print(f"\nâš ï¸ æ€§èƒ½è¦æ±‚é•å ({len(overhead_violations)} é …):")
        for violation in overhead_violations:
            print(f"   {violation['endpoint']} (ä½µç™¼ {violation['concurrent_users']}): "
                  f"{violation['mean_response_time']:.2f}ms > 20ms")
    else:
        print(f"\nâœ… æ‰€æœ‰æ¸¬è©¦éƒ½ç¬¦åˆ < 20ms æ€§èƒ½è¦æ±‚")
    
    # é«˜ä½µç™¼æ€§èƒ½åˆ†æ
    if high_concurrency_results:
        print(f"\nğŸš€ é«˜ä½µç™¼æ€§èƒ½åˆ†æ:")
        for result in high_concurrency_results:
            print(f"   {result['endpoint']} (ä½µç™¼ {result['concurrent_users']}): "
                  f"QPS {result['requests_per_second']:.1f}, "
                  f"æˆåŠŸç‡ {result['success_rate']:.1f}%")


def save_results_to_file(results: List[Dict[str, Any]]):
    """ä¿å­˜æ¸¬è©¦çµæœåˆ°æ–‡ä»¶"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"monitoring_performance_test_{timestamp}.json"
    
    test_summary = {
        "test_time": datetime.now().isoformat(),
        "test_config": TEST_CONFIG,
        "results": results,
        "summary": {
            "total_tests": len(results),
            "wbs_requirement": "< 20ms additional overhead",
            "test_passed": all(
                r.get('response_times_ms', {}).get('mean', 999) < 20 
                for r in results if 'response_times_ms' in r
            )
        }
    }
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(test_summary, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ è©³ç´°çµæœå·²ä¿å­˜è‡³: {filename}")
    except Exception as e:
        print(f"\nâŒ ä¿å­˜çµæœå¤±æ•—: {e}")


async def test_api_availability():
    """æ¸¬è©¦ API æœå‹™æ˜¯å¦å¯ç”¨"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{TEST_CONFIG['base_url']}/health", timeout=5) as response:
                if response.status == 200:
                    data = await response.json()
                    print(f"âœ… æ¸¬è©¦ API æœå‹™å¯ç”¨: {data.get('service', 'unknown')}")
                    return True
                else:
                    print(f"âŒ API æœå‹™å›æ‡‰ç•°å¸¸: HTTP {response.status}")
                    return False
    except Exception as e:
        print(f"âŒ ç„¡æ³•é€£æ¥æ¸¬è©¦ API æœå‹™: {e}")
        print(f"ğŸ’¡ è«‹ç¢ºä¿æ¸¬è©¦æœå‹™æ­£åœ¨é‹è¡Œ: python test_model_api.py")
        return False


if __name__ == "__main__":
    print("ğŸ” Model API ç›£æ§æ€§èƒ½æ¸¬è©¦")
    print(f"ğŸ“… æ¸¬è©¦æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ¯ WBS è¦æ±‚: ç›£æ§ä»£ç†å»¶é² < 20ms")
    
    async def main():
        # æª¢æŸ¥ API å¯ç”¨æ€§
        if not await test_api_availability():
            return
        
        # åŸ·è¡Œæ€§èƒ½æ¸¬è©¦
        await test_monitoring_overhead()
        
        print("\nâœ… æ¸¬è©¦å®Œæˆ")
    
    # é‹è¡Œæ¸¬è©¦
    asyncio.run(main()) 